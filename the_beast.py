import cloudscraper
import re
import csv
from bs4 import BeautifulSoup

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…ØªØµÙØ­ Ø§Ù„ÙˆÙ‡Ù…ÙŠ Ù„ØªØ¬Ø§ÙˆØ² Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹
scraper = cloudscraper.create_scraper(browser={'browser': 'chrome','platform': 'android','desktop': False})

def sniper_others_links(page_url):
    """Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© ØªØ¯Ø®Ù„ Ù„ØµÙØ­Ø© Ø§Ù„Ù…Ø³Ù„Ø³Ù„ ÙˆØªØµØ·Ø§Ø¯ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø³ÙŠØ±ÙØ±Ø§Øª Ø§Ù„ØªÙŠ Ø±ÙØ¹Ù‡Ø§ ØºÙŠØ±Ùƒ"""
    links = {"1080p": "", "720p": "", "480p": ""}
    try:
        res = scraper.get(page_url, timeout=15)
        html = res.text
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø³ÙŠØ±ÙØ±Ø§Øª Ø§Ù„Ø¬Ø§Ù‡Ø²Ø© (Uqload, Dood, Upstream)
        dood = re.findall(r'https?://(?:doodstream\.com|dood\.to|dood\.so|dood\.li)/e/([a-z0-9]+)', html)
        uqload = re.findall(r'https?://(?:uqload\.com|uqload\.co)/embed-([a-z0-9]+)', html)
        upstream = re.findall(r'https?://(?:upstream\.to|upstream\.org)/embed-([a-z0-9]+)', html)

        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø£ÙƒÙˆØ§Ø¯ Ø§Ù„Ù…ÙƒØªØ´ÙØ© Ù„Ø±ÙˆØ§Ø¨Ø· ÙƒØ§Ù…Ù„Ø© ØªØ¹Ù…Ù„ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ø£ÙˆÙ†Ù„Ø§ÙŠÙ†
        if dood: links["1080p"] = f"https://dood.to/e/{dood[0]}"
        if uqload: links["720p"] = f"https://uqload.com/embed-{uqload[0]}.html"
        if upstream: links["480p"] = f"https://upstream.to/embed-{upstream[0]}.html"
        
        return links
    except:
        return links

def update_database():
    # Ø±Ø§Ø¨Ø· Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø°ÙŠ Ø³Ù†ØµØ·Ø§Ø¯ Ù…Ù†Ù‡ (ÙŠÙ…ÙƒÙ†Ùƒ ØªØºÙŠÙŠØ±Ù‡ Ù„Ø£ÙŠ Ù…ÙˆÙ‚Ø¹ ÙŠØ¹Ø±Ø¶ Ù…Ø³Ù„Ø³Ù„Ø§Øª)
    source_url = "https://wecima.show/category/%d9%85%d8%b3%d9%84%d8%b3%d9%84%d8%a7%d8%aa-%d8%aa%d8%b1%d9%83%d9%8a%d8%a9/"
    db_file = 'database.csv'
    all_data = []

    print(f"ğŸš€ Ø§Ù„Ø¨Ø¯Ø¡ ÙÙŠ Ù‚Ù†Øµ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø³ÙŠØ±ÙØ±Ø§Øª Ù…Ù†: {source_url}")
    try:
        res = scraper.get(source_url)
        soup = BeautifulSoup(res.text, 'html.parser')
        items = soup.find_all('div', class_='GridItem')

        for item in items[:15]: # Ø³Ø­Ø¨ Ø¢Ø®Ø± 15 Ø­Ù„Ù‚Ø©
            name = item.find('strong').text.strip() if item.find('strong') else "Ø­Ù„Ù‚Ø© Ø¬Ø¯ÙŠØ¯Ø©"
            link = item.find('a')['href']
            
            print(f"ğŸ“¡ Ø¬Ø§Ø±ÙŠ ÙØ­Øµ ØµÙØ­Ø©: {name}")
            v_links = sniper_others_links(link)
            
            all_data.append({
                'name': name,
                'url_1080p': v_links['1080p'],
                'url_720p': v_links['720p'],
                'url_480p': v_links['480p']
            })

        # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ù…Ù„Ù CSV
        with open(db_file, mode='w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=['name', 'url_1080p', 'url_720p', 'url_480p'])
            writer.writeheader()
            writer.writerows(all_data)
        print("âœ… Ø§Ù„Ù…Ù‡Ù…Ø© ØªÙ…Øª! Ø§Ù„Ù…Ù„Ù Ø¬Ø§Ù‡Ø² Ø§Ù„Ø¢Ù† Ø¨Ø§Ù„Ø±ÙˆØ§Ø¨Ø·.")
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£: {e}")

if __name__ == "__main__":
    update_database()
